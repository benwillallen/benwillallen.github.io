select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=function (x) (median(x) - mean(median(x))) / sqrt(var(median(x))), .vars = vars(draftyear:VORP)) %>%
ungroup()
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup()
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP) = function (x) (x-mean(x))/sqrt(var(x)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP), function (x) (x-mean(x))/sqrt(var(x)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP), .funs=function (x) (x-mean(x))/sqrt(var(x)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP), .funs=~(.x-mean(.x))/sqrt(var(.x)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP), ~ (.x-mean(.x))/sqrt(var(.x)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP), ~ (.-mean(.))/sqrt(var(.)))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP ~ (.-mean(.))/sqrt(var(.))))
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup() %>%
mutate(across(draftyear:VORP, ~ (.-mean(.))/sd(.)))
# Splitting data into training, validation, testing, and prediction sets
# Prediction set is all players drafted in 2018 or later
pred_outcomes <- aggr_outcomes %>%
filter(draftyear >= 2018)
split_outcomes <- aggr_outcomes %>%
ungroup() %>%
filter(draftyear < 2018)
# Will use a 70-10-20 split for remaining data
train_test_split <- sample(c(TRUE, FALSE), nrow(split_outcomes), replace=TRUE, prob=c(0.8,0.2))
train_valid_split <- sample(c(TRUE, FALSE), sum(train_test_split), replace=TRUE, prob=c(0.875,0.125))
test_outcomes <- split_outcomes %>%
filter(!train_test_split)
train_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(train_valid_split)
valid_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(!train_valid_split)
train_test_split <- sample(c(TRUE, FALSE), nrow(split_outcomes), replace=TRUE, prob=c(0.8,0.2))
train_valid_split <- sample(c(TRUE, FALSE), sum(train_test_split), replace=TRUE, prob=c(0.875,0.125))
test_outcomes <- split_outcomes %>%
filter(!train_test_split)
train_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(train_valid_split)
valid_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(!train_valid_split)
pred_outcomes <- aggr_outcomes %>%
filter(draftyear >= 2018)
split_outcomes <- aggr_outcomes %>%
ungroup() %>%
filter(draftyear < 2018)
# Will use a 70-10-20 split for remaining data
train_test_split <- sample(c(TRUE, FALSE), nrow(split_outcomes), replace=TRUE, prob=c(0.8,0.2))
train_valid_split <- sample(c(TRUE, FALSE), sum(train_test_split), replace=TRUE, prob=c(0.875,0.125))
test_outcomes <- split_outcomes %>%
filter(!train_test_split)
train_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(train_valid_split)
valid_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(!train_valid_split)
# Exploratory Data Analysis
corrplot(cor(train_outcomes), cl.cex=0.5, tl.cex=0.5, method="square")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="red")
# We will want to aggregate the data for ease of use
aggr_outcomes <- season_outcomes %>%
select_if(is.numeric) %>%
group_by(nbapersonid) %>%
summarise_at(.funs=median, .vars = vars(draftyear:VORP)) %>%
ungroup()
pred_outcomes <- aggr_outcomes %>%
filter(draftyear >= 2018)
split_outcomes <- aggr_outcomes %>%
ungroup() %>%
filter(draftyear < 2018)
# Will use a 70-10-20 split for remaining data
train_test_split <- sample(c(TRUE, FALSE), nrow(split_outcomes), replace=TRUE, prob=c(0.8,0.2))
train_valid_split <- sample(c(TRUE, FALSE), sum(train_test_split), replace=TRUE, prob=c(0.875,0.125))
test_outcomes <- split_outcomes %>%
filter(!train_test_split)
train_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(train_valid_split)
valid_outcomes <- split_outcomes %>%
filter(train_test_split) %>%
filter(!train_valid_split)
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="red")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome/nrow(train_outcomes)), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="red")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome/nrow(train_outcomes)), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome/nrow(test_outcomes)), alpha=0.5, col="red")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="red")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, col="red") +
scale_y_continuous(labels = scales::label_number(scale = 0.01))
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue") +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red")
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue",bins=6) +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red",bins=6)
ggplot() +
geom_histogram(data=train_outcomes, mapping=aes(x=career_outcome/950), alpha=0.5, fill="blue", bins=6) +
geom_histogram(data=test_outcomes, mapping=aes(x=career_outcome/239), alpha=0.5, fill="red", bins=6)
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="identity") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="identity")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome, y=.density), alpha=0.5, fill="blue", stat="identity") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome, y=.density), alpha=0.5, fill="red", stat="identity")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="density") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="density")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count") +
scale_y_continuous(limits=c(0,1))
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count", width=1) +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count", width=1)
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count", position="dodge") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count") +
labs(title="Distribution of Career Outcome for Train and Test Sets", subtitle = "1 = Elite, 6 = Out Of League")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count") +
labs(title="Distribution of Career Outcome for Train and Test Sets", subtitle = "1 = Elite, 6 = Out Of League",
y="Frequency", x="Career Outcome") + theme_bw()
View(rebounding_data)
rebound_train <- rebounding_data %>%
filter(game_number < 81 & (team == "OKC" | opp_team == "OKC"))
View(rebound_train)
rebound_train <- rebounding_data %>%
filter(game_number < 81 & team == "OKC")
rebound_train <- rebounding_data %>%
filter(game_number < 81 & team == "OKC") %>%
summarise(prediction = mean(oreb_pct))
# Will use a 80-20 split for remaining data
train_test_split <- sample(c(TRUE, FALSE), nrow(split_outcomes), replace=TRUE, prob=c(0.8,0.2))
test_outcomes <- split_outcomes %>%
filter(!train_test_split)
train_outcomes <- split_outcomes %>%
filter(train_test_split)
# Exploratory Data Analysis
corrplot(cor(train_outcomes), cl.cex=0.5, tl.cex=0.5, method="square")
ggplot() +
geom_bar(data=train_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="blue", stat="count") +
geom_bar(data=test_outcomes, mapping=aes(x=career_outcome), alpha=0.5, fill="red", stat="count") +
labs(title="Distribution of Career Outcome for Train and Test Sets", subtitle = "1 = Elite, 6 = Out Of League",
y="Frequency", x="Career Outcome") + theme_bw()
filter(test_outcomes, career_outcome == 6)
length(filter(test_outcomes, career_outcome == 6)) / length(test_outcomes)
sum(filter(test_outcomes, career_outcome == 6)) / length(test_outcomes)
sum(filter(test_outcomes, career_outcome == 6))
filter(test_outcomes, career_outcome == 6)
length(filter(test_outcomes, career_outcome == 6))
filter(test_outcomes, career_outcome == 6)
nrow(filter(test_outcomes, career_outcome == 6))
nrow(filter(test_outcomes, career_outcome == 6)) / nrow(test_outcomes)
k <- 5
View(train_outcomes)
knn_model <- knn(train = train_outcomes[, "draftpick":"VORP"],  # Features in training data
test = test_outcomes[, "draftpick":"VORP"],    # Features in test data
cl = train_outcomes[, "career_outcome"],      # Class labels in training data
k = k)
knn_model <- knn(train = train_outcomes[, "mins"],  # Features in training data
test = test_outcomes[, "mins"],    # Features in test data
cl = train_outcomes[, "career_outcome"],      # Class labels in training data
k = k)
sum(is.na(train_outcomes))
train_outcomes[, "mins"]
train_outcomes[, "career_outcomes"]
train_outcomes[, "career_outcome"]
knn_model <- knn(train = train_outcomes[, "mins"],  # Features in training data
test = test_outcomes[, "mins"],    # Features in test data
cl = train_outcomes[, "career_outcome"],      # Class labels in training data
k = k)
knn_model <- knn(train = train_outcomes[, "mins"],  # Features in training data
test = test_outcomes[, "mins"],    # Features in test data
cl = as.factor(train_outcomes[, "career_outcome"]),      # Class labels in training data
k = k)
knn_model <- knn(train = train_outcomes[, "mins"],  # Features in training data
test = test_outcomes[, "mins"],    # Features in test data
cl = train_outcomes[, "career_outcome"],      # Class labels in training data
k = k)
as.factor(train_outcomes[, "career_outcome"])
knn_model <- knn(train = train_outcomes[, "mins"],  # Features in training data
test = test_outcomes[, "mins"],    # Features in test data
cl = train_outcomes[, "career_outcome"],      # Class labels in training data
k = k)
knn_model <- knn(train = train_outcomes$mins,  # Features in training data
test = test_outcomes$mins,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k)
knn_model <- knn(train = train_outcomes$mins,  # Features in training data
test = test_outcomes$mins,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prop = TRUE)
knn_model <- knn(train = train_outcomes$mins,  # Features in training data
test = test_outcomes$mins,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
dim(train_outcomes$mins)
dim(test_outcomes$mins)
typeof(train_outcomes$mins)
typeof(test_outcomes$mins)
dim(train_outcomes$mins)
nrow(train_outcomes$mins)
train_outcomes$mins
test_outcomes$mins
knn_model <- knn(train = train_outcomes,  # Features in training data
test = test_outcomes,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
true_labels <- test_outcomes$career_outcome
cat("Accuracy:", accuracy, "\n")
# Calculate accuracy
accuracy <- sum(knn_model == true_labels) / length(true_labels)
cat("Accuracy:", accuracy, "\n")
# Calculate accuracy
knn_model$prob
# Calculate accuracy
print(knn_model)
levels(knn_model)
# Calculate accuracy
print(knn_model)
attr(knn_model, "prob")
knn_probs <- attr(knn_model, "prob")
install.packages("caret")
library(caret)
confusion <- confusionMatrix(predicted_labels, true_labels)
confusion <- confusionMatrix(knn_model, true_labels)
print(knn_model)
confusion <- confusionMatrix(knn_model, true_labels)
levels(true_labels)
true_labels <- as.factor(test_outcomes$career_outcome)
levels(true_labels)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
k <- 6
knn_model <- knn(train = train_outcomes,  # Features in training data
test = test_outcomes,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs <- attr(knn_model, "prob")
true_labels <- as.factor(test_outcomes$career_outcome)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
k <- 10
knn_model <- knn(train = train_outcomes,  # Features in training data
test = test_outcomes,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs <- attr(knn_model, "prob")
true_labels <- as.factor(test_outcomes$career_outcome)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
sub_train_outcomes <- train_outcomes %>%
select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_train_outcomes <- train_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_test_outcomes <- test_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_train_outcomes <- train_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_test_outcomes <- test_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
knn_model2 <- knn(train = sub_train_outcomes,  # Features in training data
test = sub_test_outcomes,    # Features in test data
cl = sub_train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs2 <- attr(knn_model, "prob")
true_labels <- as.factor(sub_test_outcomes$career_outcome)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
true_labels <- as.factor(test_outcomes$career_outcome)
sub_train_outcomes <- train_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_test_outcomes <- test_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
knn_model2 <- knn(train = sub_train_outcomes,  # Features in training data
test = sub_test_outcomes,    # Features in test data
cl = sub_train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs2 <- attr(knn_model, "prob")
confusion <- confusionMatrix(knn_model2, true_labels)
print(confusion)
k <- 10
knn_model <- knn(train = train_outcomes,  # Features in training data
test = test_outcomes,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs <- attr(knn_model, "prob")
true_labels <- as.factor(test_outcomes$career_outcome)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
install.packages("naivebayes")
library(naivebayes)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ .)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ .)
summary(nb_model)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
summary(nb_model)
plot(nb_model)
nb_confusion <- confusionMatrix(nb_model, true_labels)
nb_pred <- predict(nb_model, test_outcomes)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(, true_labels)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
train_outcomes$career_outcome <- as.factor(train_outcomes$career_outcome)
test_outcomes$career_outcome <- as.factor(test_outcomes$career_outcome)
nb_model <- naive_bayes(data = train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_pred <- predict(nb_model, test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
train_outcomes$career_outcome <- as.factor(train_outcomes$career_outcome)
test_outcomes$career_outcome <- as.factor(test_outcomes$career_outcome)
nb_model <- naive_bayes(data = train_outcomes, career_outcome ~ ., usekernel = TRUE)
nb_pred <- predict(nb_model, test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
confusion <- confusionMatrix(knn_model2, sub_train_outcomes$career_outcome)
sub_train_outcomes <- train_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_test_outcomes <- test_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
knn_model2 <- knn(train = sub_train_outcomes,  # Features in training data
test = sub_test_outcomes,    # Features in test data
cl = sub_train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs2 <- attr(knn_model, "prob")
confusion <- confusionMatrix(knn_model2, true_labels)
print(confusion)
sub_train_outcomes <- train_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
sub_test_outcomes <- test_outcomes %>%
dplyr::select(career_outcome, draftyear, mins, fgp2, fgp3, ftp, PER, VORP)
knn_model2 <- knn(train = sub_train_outcomes,  # Features in training data
test = sub_test_outcomes,    # Features in test data
cl = sub_train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs2 <- attr(knn_model, "prob")
confusion <- confusionMatrix(knn_model2, true_labels)
print(confusion)
k <- 10
knn_model <- knn(train = train_outcomes,  # Features in training data
test = test_outcomes,    # Features in test data
cl = train_outcomes$career_outcome,      # Class labels in training data
k = k, prob = TRUE)
knn_probs <- attr(knn_model, "prob")
true_labels <- as.factor(test_outcomes$career_outcome)
confusion <- confusionMatrix(knn_model, true_labels)
print(confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
sub_train_outcomes$career_outcome <- as.factor(sub_train_outcomes$career_outcome)
sub_test_outcomes$career_outcome <- as.factor(sub_test_outcomes$career_outcome)
nb_model <- naive_bayes(data = sub_train_outcomes, career_outcome ~ ., usekernel = TRUE)
plot(nb_model)
nb_train_pred <- predict(nb_model, sub_train_outcomes)
nb_train_confusion <- confusionMatrix(nb_train_pred, sub_train_outcomes$career_outcome)
print(nb_train_confusion)
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
View(test_outcomes)
install.packages("keras")
library(keras)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = dim(train_outcomes)[2]) %>%
layer_dense(units = 6, activation = "softmax")
summary(nn_model)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = dim(train_outcomes)[2]) %>%
layer_dense(units = 6, activation = "softmax")
install_tensorflow()
nb_pred <- predict(nb_model, sub_test_outcomes)
nb_confusion <- confusionMatrix(nb_pred, true_labels)
print(nb_confusion)
View(test_outcomes)
# Standardizing data to improve performance
train_outcomes <- train_outcomes %>%
mutate(across(draftpick:VORP, ~scale(.) %>% rescale(to = c(0, 1))))
# Standardizing data to improve performance
train_outcomes <- train_outcomes %>%
mutate(across(c(draftpick:VORP), ~scale(.) %>% rescale(to = c(0, 1))))
# Standardizing data to improve performance
train_outcomes <- train_outcomes %>%
mutate(across(all_of(draftpick:VORP), ~scale(.) %>% rescale(to = c(0, 1))))
